探探Service mesh架构从2021年中开始调研验证，到年末已经实现生产20%微服务的接入，顺利通过高稳定性和高性能两项核心指标的验收。在此我们决意分享架构落地经验，以此为业内关注Service mesh架构落地的同僚提供参考。
# 探探的微服务架构演进
在2018年，探探对原本的单体服务做了微服务化拆分，并以http/gRPC作为服务间主要通讯协议。此后服务数量便持续攀升，到2021年中，已经有约1000个微服务在线上部署，注册实例数5k+。

在此过程中，为应对微服务化带来的上下游依赖复杂化以及稳定性方面的风险，我们持续对微服务架构做了演进，先后上线了服务注册中心提供服务注册发现，支持配置动态加载的Api Gateway，分布式tracing和通过静态配置实现的服务限流熔断等。

但如前面所述，我们在服务治理方向的工作并不能满足业务所需。如依赖静态配置的服务限流使研发在服务添加依赖时将不得不手动更改配置进行发版，效率非常低下。我们期望有更加强大的服务治理功能可以落地，方便研发和运维轻松地做好稳定性优化工作。

# 服务治理方案选型

要推进服务治理，现阶段一般有两种做法，一是迭代现有Go gRPC框架，二是基于Service mesh。如果继续走服务框架的方式来推进，则存在几个明显的问题：

- 业务接入周期长，项目推进缓慢

> 随着未来服务治理框架的完善，框架层面会有越来越多的bugfix和紧急feature的上线，这类变更必须依赖研发接入才可实现。而从以前的经验看，一个新的库版本要完全发布，至少需要一个月的时间。在这种情况下，服务治理功能演进无法实现快速试错和迭代，落地阻力非常明显。

- 多版本兼容问题，研发投鼠忌器

> 考虑到不同业务可能使用了框架的不同版本，如果框架变动比较大，前后两个版本无法兼容，那么研发将不得不对以前的版本做到兼容，容易引入意想不到的问题，且研发成本变高。

- 多语言治理困难，无法将所有业务纳入服务治理

> 目前服务治理团队的人力只能维护一个Go语言的common框架，而线上必定有不少非Go部署的程序，对这类服务如果服务治理不能覆盖，那么也容易变成整个架构的缺口，容易引发观测不到的故障。

因此，我们重点考虑了引入Service mesh架构的优劣势，以及方案可行性的衡量标准。

毫无疑问，走服务框架带来的上面几点问题都可以通过单独部署Sidecar的方式，将流量管控治理能力下沉来解决。但是这种方案也不是完美无缺，引入Service mesh势必会带来新的挑战：

1. **低侵入。**接入Mesh架构难免业务配合，如果涉及框架改造升级，那么研发接入的意愿就会降低，接入成本也会变高。如何在研发不改一行代码的情况下一键式接入，是我们要考虑的核心问题；
2. **架构兼容。**目前的线上业务架构已经基本成型，如果要做service mesh，必须在不影响架构稳定性的前提下，通过灰度的方式让业务接入，且不带来额外的稳定性负担；
3. **高性能。**探探的业务对接口延时较敏感，且部分核心接口涉及较长的调用链路，sidecar必须保证核心接口的延迟不能上涨过于明显；
4. **mesh架构稳定性。**线上业务对可靠性的要求极高，而mesh又是托管了几乎所有的流量，一旦故障将导致全站不可用，造成重大损失。这要求我们在落地过程中必须充分考虑自身的稳定性，避免重大故障的发生。

为此，不管是技术选型还是方案设计，我们都必须充分考虑是否会带来上述问题。
# 技术选型
### 控制面选型
要说Service mesh控制面选型，肯定是绕不开istio的。诚然，istio是服务网格领域最受推崇的项目之一，社区活跃，各大厂也都积极参与到了其中的建设。我们也认真调研了istio的实现方案作为预备方案之一，但是最终并未采用。

在istio的实现里，Sidecar CRD定义了一个微服务所依赖的服务项。istio查找K8s的service或service entry来确定这些服务的端口号，并按端口聚合成xDS配置。配合iptables重定向，访问特定端口的流量就能重定向到envoy。

如果流量劫持采用istio社区推荐的iptables方案，那就意味着所有进出pod的流量都会被劫持，且受到iptables性能的约束。访问prometheus、pg、redis的流量也会因为增加一跳带来不必要的延迟开销。最关键的，一旦sidecar故障，想要修改iptables规则禁用流量劫持是非常困难的。因此，我们排除了iptables透明劫持方案。

不用重定向，那就只能在服务发现处下手了。我们想到可以利用注册中心下发回环ip+sidecar outbound端口来劫持流量。但是这样一来，访问服务的目标端口信息就不存在了，也就无法再基于istio的xDS配置将流量转发出去。

此外，istio还存在不少隐形问题：

1. 出于历史原因，探探各服务端口是尽量唯一的，而istio的方案限制了端口数量；
2. 全局统一的CRD配置在细粒度控制单一链路的流量时显得捉襟见肘；
3. 我们认为envoy filter不算是个好的实现，不能满足后续的扩展性需要；
4. 无法对配置做灰度控制，修改一个CRD资源会导致全站更新，影响面不可控；
5. 集群规模大时，存在性能瓶颈问题；

由于istio的这些问题，想要在此基础上进行改造开发，成本确实不可承受。我们的需求始终只是一个Sidecar的配置中心。因此，经过最终讨论，我们决定参考istio自研控制面。
### 数据面选型
在数据面选择上，我们分别调研了envoy和mosn。envoy数据面虽然成熟稳定，但本身是C++开发的，而我们使用envoy必定涉及不少代码的定制化开发工作，长久来看这对本身技术栈是Go的探探来说成本太高，因此我们采用了istio官方推荐的另一个数据面mosn。它基于Go实现，由蚂蚁金服开源，已在蚂蚁集团大规模落地使用。

# 整体架构 
![](/images/kubernetes/WechatIMG497.png)

- **Control Plane控制面**，负责路由管理，治理策略的存储和发现；
	- **Galley**，提供治理策略的创建/修改/发布接口；
	- **Pilot**，提供数据面策略发现接口；
- **Mosn**，与业务进程部署在同一Pod内，通过回环ip与业务进程交互，代理转发进出流量；
- **服务注册中心**，负责提供服务注册/发现、透明劫持、mesh降级能力；
- **TCP平台**，提供治理策略的变更操作页面，提供sidecar运维接口；
- **混合云**，提供sidecar管理/发布的操作接口。

以下将围绕流量劫持和Sidecar容灾方案、无损业务发布/Sidecar升级方案、控制面配置管理方案三个方向，向大家分享探探mesh架构的关键技术要点。
## 流量劫持和Sidecar容灾方案
业界对透明流量劫持的方案有不同的实现。像istio是通过iptables的NAT表实现透明劫持，阿里是通过iptables的mangle表实现，百度是通过本地的naming agent下发回环地址劫持，美团是使用Unix Domain Socket方式，打通SDK和proxy来实现转发。社区的方案因为修改iptables的方案性能和运维成本均比较高，在探探落地较为困难。阿里和美团的方案则研发成本太高。百度的naming agent下发回环地址方案有一定的借鉴价值，因为我们大部分业务服务都是走内部注册中心做服务发现的，服务注册中心本身可以提供下发回环地址或代理端口以透明劫持服务流量的能力。 

思路：基于本地loopbackip方案，打通Mosn与服务注册中心，通过服务注册中心下发本地loopbackip劫持outbound流量，通过下发Mosn端口劫持inbound流量。同时，Mosn对服务注册中心做心跳保活，一旦Mosn挂掉可自动切换到直连。在异常情况下，还可以通过服务注册中心的降级接口，将流量切换到直连，避免Mosn故障导致流量损失。

![](/images/kubernetes/WechatIMG502.png)

针对非Go的业务，我们提供了基于DNS的访问方式，通过指定服务名，dns解析到本地loopbackip实现流量转发。
通过以上方式，我们实现了业务模块无需修改代码即可接入mesh的低侵入方式，降低了业务接入成本。

## 无损业务发布/Sidecar升级方案
### 业务无损发布
在Pod没有接入Sidecar时，删除Pod实例不会导致流量有损，因为对于http/gRPC框架，在调用优雅关闭接口时，会先关闭监听的listener，再对现存的连接发送go away frame或Connection:closed Header，不影响已经建立的请求。新请求因为连接无法建立，就不会再向被删除实例发送请求了。而在Sidecar代理后，问题就变的复杂了。

首先，删除Pod时，如果Sidecar容器和业务容器是同时退出的，那么sidecar outbound端口就会关闭，业务进程中仍然存在的访问外部的请求就会因为sidecar端口访问不通而请求失败，造成业务有损。

我们的做法是，在sidecar容器的preStop Hook中配置脚本，探测业务进程debug端口的监听情况，如果端口不再监听，认为业务进程已经退出，可以安全的关闭sidecar进程。在一些没有debug端口的业务中，默认等待5s后关闭。

![](/images/kubernetes/WechatIMG501.png)

其次，因为Sidecar代理了pod的inbound流量，在删除pod时，如果Sidecar端口存活，而只是关闭业务监听端口，无法阻止client继续向pod发送请求。这也会导致流量有损问题。要保证业务容器关闭时，访问服务的client不再向待删除的pod发起新的请求。

我们的做法是，在业务容器的preStop Hook中调用业务debug端口发送delete请求，等待5s后再关闭业务进程。在sidecar中，配置1s间隔的主动健康检查机制，探测代理Pod的debug端口健康状态，如果发现返回的状态是unhealthy，再关闭监听的inbound端口，并对现存的连接做平滑关闭操作。

![](/images/kubernetes/WechatIMG503.png)

### Sidecar无损升级

## 配置版本控制和自动加载方案
istio的配置下发是无法做到按版本控制和按实例灰度发布的，而实际上在生产环境的改动影响面是难以事先估量的，如果不加以灰度控制和回滚，一旦配置错误可能导致全局故障，为此我们的控制面实现了以下特性：

1. 不同服务的配置隔离；
3. 支持灰度配置下发和回滚。

![](/iamges/kubernetes/WechatIMG505.png)

上图描述了service mesh配置在etcd中的组织形式，以及版本发布的实现机制。配置存储以服务为粒度，所以以服务为前缀的key存储了这个服务所用到的所有mesh配置，包括监听的listener、virtual_service、destination_rule。为了支持多版本并行发布（应对后续自动化运维的自动发布需求），设计了MVCC的键，存储namespace的最新发布版本id和正在发布中的版本id。创建版本时，新增一个版本的key记录这个版本的所有变更记录。选择节点发布时，更新节点的期望版本，pilot watch到节点期望变更时，自动同步最新变更的版本配置下发到端。灰度直至所有节点更新到最新版本后，自动合并版本的变更记录到namespace下的配置中。

istio通过sidecar CRD定义了一个服务依赖的服务列表，而在实际生产环境，业务以来的服务项特别多，要每次新增依赖都手动添加配置，对业务而言成本是比较高的。所以我们设计了自动配置发布功能。在服务A访问服务B时，如果sidecar发现没有服务B的路由信息，会使用passthrough cluster透传请求到服务B的实例上。但passthrough的方式不能长久使用，否则不利于流量治理和监控。因此，流量经过passthrough cluster时，sidecar会触发控制面api回调，异步调用控制面接口创建依赖配置并下发。基于这个机制，可以实现业务无需关注服务依赖即可自动添加配置，大大降低了研发接入和使用成本。

![](/images/kubernetes/WechatIMG504.png)

# 总结
从18年开始探探启动了微服务的拆分，到如今service mesh架构落地，是公司向云原生方向迈进的又一个里程碑。未来，我们将基于K8s和service mesh平台，将更多的分布式能力融入底层基础设施，为各业务线提供低侵入、低成本、标准化的服务治理解决方案。